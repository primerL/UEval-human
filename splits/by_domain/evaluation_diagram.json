[
  {
    "id": 13,
    "domain": "diagram",
    "question": "I am preparing a figure for my paper to illustrate the following framework:\nThis figure showcases AutoPresent, a system for generating presentation slides directly from natural language instructions. The system interprets user instructions and produces slides with precise formatting and content, including text, images, backgrounds, and layout.\n\n**Main Components**:\n- **Input**: Natural language command, e.g., \"Present Airbnb's misión with a mission statement and a relevant image, for English and Spanish audience.\"\n- **Code Generation**: A code snippet is auto-generated using a Python `pptx` library. The functions include:\n  - `add_title()` to insert slide titles.\n  - `add_text()` for detailed mission statement content.\n  - `search_image()` and `generate_image()` to insert related visuals.\n  - `set_background_color()` for visual styling.\n- **Slide Output Examples**:\n  - A bilingual mission slide with retrieved image and color styling.\n  - Slides created with titles, diagrams (via `add_shape()`), web snapshots (via `take_snapshot()`), and rich layouts.\n\n**Key Functions Highlighted**: Each visual element is annotated with the corresponding API call that generated it (e.g., `add_title`, `search_image`, `add_shape(box)`), illustrating the full visual design pipeline.\n\nPlease generate:\n 1. A diagram showing how natural language maps to structured presentation components.\n 2. A separate caption summarizing the key visual flow from instruction to layout and content rendering.",
    "reference_text": "Automatically generating slides from natural language instructions. We propose AutoPresent, a tool-augmented code generation method that follows natural language instructions to design slides from scratch, as shown in the examples. This allows for precise control over all elements, including textual content, images, visual layouts, coloring, and more.",
    "reference_images": [
      "data/diagram/AutoPresent-Overview.png"
    ],
    "generated_text": "AutoPresent maps a natural-language instruction to structured slide components and renders them as finished slides. The pipeline proceeds left-to-right: (1) **Input** parses the user request and detects targets (e.g., title, mission text, image, bilingual audience). (2) **Interpretation & Planning** expands the request into a slide schema (title, text blocks, image slot, background color, layout grid, language variants). (3) **Code Generation** emits Python `pptx` calls that realize each element—`add_title()` for headings, `add_text()` for mission content, `search_image()`/`generate_image()` for visuals, `set_background_color()` for styling, `add_shape()` for diagrams, and `take_snapshot()` for web snapshots. (4) **Rendering** assembles assets with the pptx renderer and validates formatting. (5) **Slide Outputs** show examples: a bilingual mission slide (EN/ES) with retrieved image and color styling, a diagram slide built via `add_shape()`, and a web-snapshot slide produced by `take_snapshot()`. Each visual element is annotated with the API call that created it, illustrating the full path from instruction to layout and content rendering.",
    "generated_image": "GPT-thinking/diagram/13.png",
    "gemini_evaluation": {
      "Question": "I am preparing a figure for my paper to illustrate the following framework:\nThis figure showcases AutoPresent, a system for generating presentation slides directly from natural language instructions. The system interprets user instructions and produces slides with precise formatting and content, including text, images, backgrounds, and layout.\n\n**Main Components**:\n- **Input**: Natural language command, e.g., \"Present Airbnb's misión with a mission statement and a relevant image, for English and Spanish audience.\"\n- **Code Generation**: A code snippet is auto-generated using a Python `pptx` library. The functions include:\n  - `add_title()` to insert slide titles.\n  - `add_text()` for detailed mission statement content.\n  - `search_image()` and `generate_image()` to insert related visuals.\n  - `set_background_color()` for visual styling.\n- **Slide Output Examples**:\n  - A bilingual mission slide with retrieved image and color styling.\n  - Slides created with titles, diagrams (via `add_shape()`), web snapshots (via `take_snapshot()`), and rich layouts.\n\n**Key Functions Highlighted**: Each visual element is annotated with the corresponding API call that generated it (e.g., `add_title`, `search_image`, `add_shape(box)`), illustrating the full visual design pipeline.\n\nPlease generate:\n\t1. A diagram showing how natural language maps to structured presentation components.\n\t2. A separate caption summarizing the key visual flow from instruction to layout and content rendering.",
      "Evaluation": {
        "Correctness": [
          4,
          "The generated image and text correctly depict the requested framework. The diagram accurately shows the flow from natural language input to slide outputs, and it includes all the specified components and function annotations. The only inaccuracy is that the text within the diagram's example slides and input box is illegible placeholder text, not the actual content described in the prompt (e.g., 'Airbnb's misión')."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text perfectly describes the generated image. It follows the five-step pipeline shown in the diagram, explaining each stage (Input, Interpretation, Code Generation, Rendering, Outputs) in detail. The text accurately references the visual elements and annotations present in the image, creating a very strong and coherent connection between the two."
        ],
        "Content Quality": [
          3,
          "The text quality is excellent—it is well-written, clear, and professional. However, the image quality is significantly compromised by the use of garbled, unreadable text for all the content examples. While the overall layout and structure of the diagram are good, the placeholder text makes it look unfinished and reduces its utility as a final figure."
        ],
        "Human Preference Alignment": [
          5,
          "The generated response is highly aligned with the user's request for a professional figure for a paper. The style is clean, the diagram is logical, and the caption is informative. It successfully avoids any undesirable content and presents the information in a formal, academic tone."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both the requested diagram and the separate caption. All the components and functions specified in the prompt (input, code generation, specific slide examples, annotations) are included in both the visual and textual explanations."
        ],
        "Content Richness": [
          4,
          "The response is quite rich, adding logical intermediate steps like 'Interpretation & Planning' and 'Rendering' that were not in the reference, making the pipeline more detailed. The text provides a thorough explanation. The only minor drawback is that the 'Code Generation' step in the image lists functions rather than showing a code snippet, and the examples use placeholder text, which is less rich than showing the actual content."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I am preparing a figure for my paper to illustrate the following framework:\\nThis figure showcases AutoPresent, a system for generating presentation slides directly from natural language instructions. The system interprets user instructions and produces slides with precise formatting and content, including text, images, backgrounds, and layout.\\n\\n**Main Components**:\\n- **Input**: Natural language command, e.g., \\\"Present Airbnb's misión with a mission statement and a relevant image, for English and Spanish audience.\\\"\\n- **Code Generation**: A code snippet is auto-generated using a Python `pptx` library. The functions include:\\n  - `add_title()` to insert slide titles.\\n  - `add_text()` for detailed mission statement content.\\n  - `search_image()` and `generate_image()` to insert related visuals.\\n  - `set_background_color()` for visual styling.\\n- **Slide Output Examples**:\\n  - A bilingual mission slide with retrieved image and color styling.\\n  - Slides created with titles, diagrams (via `add_shape()`), web snapshots (via `take_snapshot()`), and rich layouts.\\n\\n**Key Functions Highlighted**: Each visual element is annotated with the corresponding API call that generated it (e.g., `add_title`, `search_image`, `add_shape(box)`), illustrating the full visual design pipeline.\\n\\nPlease generate:\\n\\t1. A diagram showing how natural language maps to structured presentation components.\\n\\t2. A separate caption summarizing the key visual flow from instruction to layout and content rendering.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            4,\n            \"The generated image and text correctly depict the requested framework. The diagram accurately shows the flow from natural language input to slide outputs, and it includes all the specified components and function annotations. The only inaccuracy is that the text within the diagram's example slides and input box is illegible placeholder text, not the actual content described in the prompt (e.g., 'Airbnb's misión').\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text perfectly describes the generated image. It follows the five-step pipeline shown in the diagram, explaining each stage (Input, Interpretation, Code Generation, Rendering, Outputs) in detail. The text accurately references the visual elements and annotations present in the image, creating a very strong and coherent connection between the two.\"\n        ],\n        \"Content Quality\": [\n            3,\n            \"The text quality is excellent—it is well-written, clear, and professional. However, the image quality is significantly compromised by the use of garbled, unreadable text for all the content examples. While the overall layout and structure of the diagram are good, the placeholder text makes it look unfinished and reduces its utility as a final figure.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The generated response is highly aligned with the user's request for a professional figure for a paper. The style is clean, the diagram is logical, and the caption is informative. It successfully avoids any undesirable content and presents the information in a formal, academic tone.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both the requested diagram and the separate caption. All the components and functions specified in the prompt (input, code generation, specific slide examples, annotations) are included in both the visual and textual explanations.\"\n        ],\n        \"Content Richness\": [\n            4,\n            \"The response is quite rich, adding logical intermediate steps like 'Interpretation & Planning' and 'Rendering' that were not in the reference, making the pipeline more detailed. The text provides a thorough explanation. The only minor drawback is that the 'Code Generation' step in the image lists functions rather than showing a code snippet, and the examples use placeholder text, which is less rich than showing the actual content.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 46,
    "domain": "diagram",
    "question": "I'm illustrating the key distinction between traditional post-processing fusion of VLM and end-to-end models versus our proposed synergistic fusion framework.\n\n(a) **Previous Method**: An image sequence is processed by a Vision-Language Model (VLM) to produce an initial trajectory. This trajectory is then refined via an end-to-end model through post-processing.\n\n(b) **Our Method (SOLVE)**: The image sequence is passed through a shared visual encoder, producing inputs for both the VLM and the end-to-end model. Their respective predicted trajectories are then fused via a Synergy Module, enabling both **feature-level** and **trajectory-level** synergy.\n\nThe figure should clearly contrast the two pipelines, and use arrows to indicate flow of data and intermediate outputs.\n\nPlease provide:\n1. A clear, side-by-side diagram contrasting the two approaches.\n2. A caption summarizing the difference. Output the image and caption separately.",
    "reference_text": "Figure 1. Previous methods combine VLM and end-to-end networks through post-processing, while our method combines VLM and end-to-end networks through both feature-level synergy (shared visual encoder) and trajectory-level synergy.",
    "reference_images": [
      "data/diagram/SOLVE-Synergy-Architecture.png"
    ],
    "generated_text": "Previous post-processing fusion (a) runs VLM → trajectory → end-to-end refinement sequentially with no shared features; SOLVE (b) uses a shared visual encoder with parallel VLM and end-to-end heads whose features and predicted trajectories are fused by a Synergy Module, enabling both feature-level and trajectory-level synergy for a single fused trajectory.",
    "generated_image": "GPT-thinking/diagram/46.png",
    "gemini_evaluation": {
      "Question": "I'm illustrating the key distinction between traditional post-processing fusion of VLM and end-to-end models versus our proposed synergistic fusion framework.\n\n(a) **Previous Method**: An image sequence is processed by a Vision-Language Model (VLM) to produce an initial trajectory. This trajectory is then refined via an end-to-end model through post-processing.\n\n(b) **Our Method (SOLVE)**: The image sequence is passed through a shared visual encoder, producing inputs for both the VLM and the end-to-end model. Their respective predicted trajectories are then fused via a Synergy Module, enabling both **feature-level** and **trajectory-level** synergy.\n\nThe figure should clearly contrast the two pipelines, and use arrows to indicate flow of data and intermediate outputs.\n\nPlease provide:\n1. A clear, side-by-side diagram contrasting the two approaches.\n2. A caption summarizing the difference. Output the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both completely correct. The diagram accurately depicts the two distinct architectural pipelines described in the prompt: (a) a sequential post-processing method and (b) a parallel method with a shared encoder and a fusion module. The text accurately summarizes these diagrams and the core concepts."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text is an excellent and precise description of the generated image. Every component mentioned in the caption (shared visual encoder, parallel VLM and end-to-end heads, Synergy Module, sequential vs. parallel flow) is clearly and accurately represented in the corresponding parts of the diagram."
        ],
        "Content Quality": [
          5,
          "The image is a high-quality, professional-looking technical diagram. It is clear, well-labeled, and easy to understand. The text is also of high quality: it is well-written, grammatically correct, concise, and uses appropriate technical terminology."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with the user's request. It provides a technical illustration and caption that is professional, safe, and directly addresses the prompt without any undesirable content."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both the requested side-by-side diagram and a separate caption, fulfilling all parts of the user's request."
        ],
        "Content Richness": [
          5,
          "The response is very rich. The diagram effectively uses layout and labels to convey complex information clearly. The text is more detailed and descriptive than the reference text, providing a clearer explanation of the two methods by explicitly mentioning concepts like 'no shared features' and 'parallel heads', which enhances understanding."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm illustrating the key distinction between traditional post-processing fusion of VLM and end-to-end models versus our proposed synergistic fusion framework.\\n\\n(a) **Previous Method**: An image sequence is processed by a Vision-Language Model (VLM) to produce an initial trajectory. This trajectory is then refined via an end-to-end model through post-processing.\\n\\n(b) **Our Method (SOLVE)**: The image sequence is passed through a shared visual encoder, producing inputs for both the VLM and the end-to-end model. Their respective predicted trajectories are then fused via a Synergy Module, enabling both **feature-level** and **trajectory-level** synergy.\\n\\nThe figure should clearly contrast the two pipelines, and use arrows to indicate flow of data and intermediate outputs.\\n\\nPlease provide:\\n1. A clear, side-by-side diagram contrasting the two approaches.\\n2. A caption summarizing the difference. Output the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both completely correct. The diagram accurately depicts the two distinct architectural pipelines described in the prompt: (a) a sequential post-processing method and (b) a parallel method with a shared encoder and a fusion module. The text accurately summarizes these diagrams and the core concepts.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text is an excellent and precise description of the generated image. Every component mentioned in the caption (shared visual encoder, parallel VLM and end-to-end heads, Synergy Module, sequential vs. parallel flow) is clearly and accurately represented in the corresponding parts of the diagram.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The image is a high-quality, professional-looking technical diagram. It is clear, well-labeled, and easy to understand. The text is also of high quality: it is well-written, grammatically correct, concise, and uses appropriate technical terminology.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with the user's request. It provides a technical illustration and caption that is professional, safe, and directly addresses the prompt without any undesirable content.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both the requested side-by-side diagram and a separate caption, fulfilling all parts of the user's request.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The response is very rich. The diagram effectively uses layout and labels to convey complex information clearly. The text is more detailed and descriptive than the reference text, providing a clearer explanation of the two methods by explicitly mentioning concepts like 'no shared features' and 'parallel heads', which enhances understanding.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 109,
    "domain": "diagram",
    "question": "I am preparing a figure for my paper to illustrate the following framework:\nAnnaAgent is an emotional and cognitive dynamic agent system designed for realistic seeker simulation in psychological counseling. It addresses two key challenges: (1) Dynamic Evolution, which models seekers’ fluctuating emotions and chief complaints within a single session using an emotion inferencer, perturber, and complaint elicitor; (2) Multi-session Memory, which schedules real-time, short-term, and long-term memories across sessions using a tertiary memory mechanism. The framework integrates modules such as emotion modulation, complaint chain generation, and memory scheduling to simulate realistic seeker behaviors over multiple counseling sessions.\nPlease generate:\n\t1.\tA system diagram showing the interaction between dynamic evolution agents (emotion modulation, complaint elicitation) and multi-session memory agents (real-time, short-term, long-term memory).\n\t2.\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.",
    "reference_text": "Figure 2: The overall structure of AnnaAgent. Two groups of agents are used to control dynamic evolution and schedule multi-session memories. The upper part simulates emotional and complaint evolution, while the lower part coordinates real-time, short-term, and long-term memory across sessions for realistic seeker simulation.",
    "reference_images": [
      "data/diagram/AnnaAgent-framework.png"
    ],
    "generated_text": "# Figure caption\n\n**Figure X. AnnaAgent framework.** The system couples a turn-level **Dynamic Evolution** process—where counselor input and memory context drive **emotion modulation** (via an inferencer and perturber) and **complaint elicitation** (yielding a complaint chain and updated seeker state)—with a **tertiary Multi-session Memory** composed of **real-time**, **short-term**, and **long-term** stores governed by a memory scheduler. Reads/writes condition each next turn and persist across sessions, producing realistic within-session emotional trajectories and cross-session continuity.\n",
    "generated_image": "GPT-thinking/diagram/109.png",
    "gemini_evaluation": {
      "Question": "I am preparing a figure for my paper to illustrate the following framework:\nAnnaAgent is an emotional and cognitive dynamic agent system designed for realistic seeker simulation in psychological counseling. It addresses two key challenges: (1) Dynamic Evolution, which models seekers’ fluctuating emotions and chief complaints within a single session using an emotion inferencer, perturber, and complaint elicitor; (2) Multi-session Memory, which schedules real-time, short-term, and long-term memories across sessions using a tertiary memory mechanism. The framework integrates modules such as emotion modulation, complaint chain generation, and memory scheduling to simulate realistic seeker behaviors over multiple counseling sessions.\nPlease generate:\n\t1.\tA system diagram showing the interaction between dynamic evolution agents (emotion modulation, complaint elicitation) and multi-session memory agents (real-time, short-term, long-term memory).\n\t2.\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both highly accurate. The diagram correctly identifies and places all the key components mentioned in the prompt (Dynamic Evolution, Multi-session Memory, emotion inferencer/perturber, complaint elicitor, memory types, scheduler) and shows their interactions logically. The text caption accurately summarizes the entire framework shown in the diagram."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text caption perfectly describes the generated image. Every key term and process mentioned in the text (e.g., Dynamic Evolution, emotion modulation, complaint elicitation, tertiary memory, read/writes) is clearly depicted in the diagram, creating a strong and cohesive explanation."
        ],
        "Content Quality": [
          5,
          "The quality is excellent. The image is a clear, professional, and well-organized system diagram that is easy to understand. The text is well-written, concise, grammatically correct, and uses appropriate technical language, making it suitable for a research paper."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with the user's request. It provides a professional-looking technical diagram and a suitable caption, with no undesirable or inappropriate content. The aesthetic of the diagram is clean and effective."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It successfully delivered both of the requested items: a system diagram and a separate caption."
        ],
        "Content Richness": [
          5,
          "The response is very rich in detail. The diagram not only includes all the required components but also adds informative labels like 'Turn-level', 'Read', and 'Write' that clarify the system's operation. The text caption is dense with information, effectively summarizing the complex interactions within the framework in a single, coherent paragraph."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I am preparing a figure for my paper to illustrate the following framework:\\nAnnaAgent is an emotional and cognitive dynamic agent system designed for realistic seeker simulation in psychological counseling. It addresses two key challenges: (1) Dynamic Evolution, which models seekers’ fluctuating emotions and chief complaints within a single session using an emotion inferencer, perturber, and complaint elicitor; (2) Multi-session Memory, which schedules real-time, short-term, and long-term memories across sessions using a tertiary memory mechanism. The framework integrates modules such as emotion modulation, complaint chain generation, and memory scheduling to simulate realistic seeker behaviors over multiple counseling sessions.\\nPlease generate:\\n\\t1.\\tA system diagram showing the interaction between dynamic evolution agents (emotion modulation, complaint elicitation) and multi-session memory agents (real-time, short-term, long-term memory).\\n\\t2.\\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both highly accurate. The diagram correctly identifies and places all the key components mentioned in the prompt (Dynamic Evolution, Multi-session Memory, emotion inferencer/perturber, complaint elicitor, memory types, scheduler) and shows their interactions logically. The text caption accurately summarizes the entire framework shown in the diagram.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text caption perfectly describes the generated image. Every key term and process mentioned in the text (e.g., Dynamic Evolution, emotion modulation, complaint elicitation, tertiary memory, read/writes) is clearly depicted in the diagram, creating a strong and cohesive explanation.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The quality is excellent. The image is a clear, professional, and well-organized system diagram that is easy to understand. The text is well-written, concise, grammatically correct, and uses appropriate technical language, making it suitable for a research paper.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with the user's request. It provides a professional-looking technical diagram and a suitable caption, with no undesirable or inappropriate content. The aesthetic of the diagram is clean and effective.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It successfully delivered both of the requested items: a system diagram and a separate caption.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The response is very rich in detail. The diagram not only includes all the required components but also adds informative labels like 'Turn-level', 'Read', and 'Write' that clarify the system's operation. The text caption is dense with information, effectively summarizing the complex interactions within the framework in a single, coherent paragraph.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 45,
    "domain": "diagram",
    "question": "I'm creating a figure for my paper to illustrate how the OmniDocBench dataset was constructed.\nThe figure should show two main processes:\n1. **Data Acquisition**: 200k PDFs are sourced from the web and internal repositories. From this, 6k visually diverse pages are sampled using feature clustering, and ~1k pages are selected with attribute labels via manual balancing.\n2. **Data Annotation**: A 3-stage annotation pipeline is used. In stage 1, state-of-the-art vision models automatically annotate selected pages. In stage 2, human annotators verify and correct the annotations. In stage 3, PhD-level experts inspect and refine the results. Annotations include layout detection (bbox, attributes, reading order, affiliations) and content recognition (text, formulas, tables).\nPlease provide:\n 1. A clear diagram showing both processes.\n 2. A figure caption describing the figure. Output the image and caption separately.",
    "reference_text": "Figure 3. Overview of the OmniDocBench dataset construction.",
    "reference_images": [
      "data/diagram/OmniDocBench-Construction.png"
    ],
    "generated_text": "# Figure Caption (text only)**Figure X. OmniDocBench construction pipeline.** The figure summarizes two processes. **Data Acquisition (left):** ~200k PDFs are collected from the web and internal repositories. Using feature-based clustering to promote visual diversity, a **6k-page** subset is sampled, from which **~1k pages** with attribute labels are chosen via manual balancing for annotation. **Data Annotation (right):** a 3-stage pipeline—(**Stage 1**) automatic annotations by state-of-the-art vision models; (**Stage 2**) human verification and correction; (**Stage 3**) PhD-level expert review and refinement. Final labels cover **layout detection** (bounding boxes, attributes, reading order, affiliations) and **content recognition** (text, mathematical formulas, tables).",
    "generated_image": "GPT-thinking/diagram/45.png",
    "gemini_evaluation": {
      "Question": "I'm creating a figure for my paper to illustrate how the OmniDocBench dataset was constructed.\nThe figure should show two main processes:\n1. **Data Acquisition**: 200k PDFs are sourced from the web and internal repositories. From this, 6k visually diverse pages are sampled using feature clustering, and ~1k pages are selected with attribute labels via manual balancing.\n2. **Data Annotation**: A 3-stage annotation pipeline is used. In stage 1, state-of-the-art vision models automatically annotate selected pages. In stage 2, human annotators verify and correct the annotations. In stage 3, PhD-level experts inspect and refine the results. Annotations include layout detection (bbox, attributes, reading order, affiliations) and content recognition (text, formulas, tables).\nPlease provide:\n\t1. A clear diagram showing both processes.\n\t2. A figure caption describing the figure. Output the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          4,
          "The generated text and the overall structure of the image are correct and accurately reflect the prompt. However, the image contains several spelling errors (e.g., 'Boundir box' instead of 'Bounding box', 'affilistion' instead of 'affiliation'), which slightly reduces its correctness."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text caption perfectly describes every component and step shown in the generated image. There is a strong and accurate correspondence between the visual elements and the textual explanation."
        ],
        "Content Quality": [
          3,
          "The text caption is well-written and clear. The image, while structurally correct, has a lower quality. The layout is somewhat cluttered, the icons are generic, and most importantly, it contains multiple spelling mistakes within the diagram's labels, which is a significant quality issue for a figure intended for a paper."
        ],
        "Human Preference Alignment": [
          5,
          "The response is helpful, safe, and directly addresses the user's request. The diagrammatic format is appropriate for the task."
        ],
        "Completeness": [
          5,
          "The response is complete, providing both the requested diagram and a separate figure caption. All information from the prompt has been included."
        ],
        "Content Richness": [
          4,
          "The response is quite rich. The diagram effectively visualizes the complex process with icons and flow arrows. The text caption is detailed and provides a comprehensive summary of the figure, arguably more so than the reference caption."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm creating a figure for my paper to illustrate how the OmniDocBench dataset was constructed.\\nThe figure should show two main processes:\\n1. **Data Acquisition**: 200k PDFs are sourced from the web and internal repositories. From this, 6k visually diverse pages are sampled using feature clustering, and ~1k pages are selected with attribute labels via manual balancing.\\n2. **Data Annotation**: A 3-stage annotation pipeline is used. In stage 1, state-of-the-art vision models automatically annotate selected pages. In stage 2, human annotators verify and correct the annotations. In stage 3, PhD-level experts inspect and refine the results. Annotations include layout detection (bbox, attributes, reading order, affiliations) and content recognition (text, formulas, tables).\\nPlease provide:\\n\\t1. A clear diagram showing both processes.\\n\\t2. A figure caption describing the figure. Output the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            4,\n            \"The generated text and the overall structure of the image are correct and accurately reflect the prompt. However, the image contains several spelling errors (e.g., 'Boundir box' instead of 'Bounding box', 'affilistion' instead of 'affiliation'), which slightly reduces its correctness.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text caption perfectly describes every component and step shown in the generated image. There is a strong and accurate correspondence between the visual elements and the textual explanation.\"\n        ],\n        \"Content Quality\": [\n            3,\n            \"The text caption is well-written and clear. The image, while structurally correct, has a lower quality. The layout is somewhat cluttered, the icons are generic, and most importantly, it contains multiple spelling mistakes within the diagram's labels, which is a significant quality issue for a figure intended for a paper.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is helpful, safe, and directly addresses the user's request. The diagrammatic format is appropriate for the task.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is complete, providing both the requested diagram and a separate figure caption. All information from the prompt has been included.\"\n        ],\n        \"Content Richness\": [\n            4,\n            \"The response is quite rich. The diagram effectively visualizes the complex process with icons and flow arrows. The text caption is detailed and provides a comprehensive summary of the figure, arguably more so than the reference caption.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 78,
    "domain": "diagram",
    "question": "We propose a lightweight visual prompt tuning method, called Visual Fourier Prompt Tuning (VFPT), to improve efficiency and expressiveness in transformer-based visual models. VFPT introduces 2D Fast Fourier Transform (FFT) into partial visual prompts along both hidden and sequence dimensions. The design is compared with original Visual Prompt Tuning (VPT).\nPlease provide:\n 1. A clear figure illustrating: (a) VPT structure, (b) 2D FFT over prompts, (c) VFPT integration into transformer encoder.\n 2. A concise figure caption. Output the image and caption separately.",
    "reference_text": "Figure 1: **Overview of VPT vs. VFPT (ours) frameworks.** (a) Original Visual Prompt Tuning. (b) 2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence length dimensions. (c) The overall architecture of our proposed VFPT (see §3.2).",
    "reference_images": [
      "data/diagram/VFPT-VPT_vs_FFTPrompt.png"
    ],
    "generated_text": "Figure caption (separate text):\n\nVisual Fourier Prompt Tuning (VFPT). (a) Visual Prompt Tuning (VPT): a small set of learnable prompt tokens is prepended to the image patch tokens and fed to a frozen transformer encoder. (b) VFPT core: a partial subset of the prompt matrix (sequence × hidden) is transformed with a 2D FFT, modulated in the frequency domain, and mapped back by an inverse 2D FFT to yield enhanced prompts. (c) VFPT integration: the enhanced partial prompts are injected at selected encoder layers, enabling lightweight parameter cost with improved expressiveness over standard VPT.\n",
    "generated_image": "GPT-thinking/diagram/78.png",
    "gemini_evaluation": {
      "Question": "We propose a lightweight visual prompt tuning method, called Visual Fourier Prompt Tuning (VFPT), to improve efficiency and expressiveness in transformer-based visual models. VFPT introduces 2D Fast Fourier Transform (FFT) into partial visual prompts along both hidden and sequence dimensions. The design is compared with original Visual Prompt Tuning (VPT).\nPlease provide:\n 1. A clear figure illustrating: (a) VPT structure, (b) 2D FFT over prompts, (c) VFPT integration into transformer encoder.\n 2. A concise figure caption. Output the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both factually correct. The diagrams accurately represent the concepts of Visual Prompt Tuning (VPT), the 2D Fast Fourier Transform (FFT) process on prompts, and the integration of the resulting Visual Fourier Prompt Tuning (VFPT) prompts into a transformer encoder, consistent with the reference."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text caption perfectly aligns with the generated image. Each part of the caption, (a), (b), and (c), corresponds directly and accurately to the respective sections of the diagram, providing a clear explanation of the visual elements."
        ],
        "Content Quality": [
          5,
          "The image quality is excellent for a technical diagram. It is clear, well-organized, and uses a consistent and professional style. The text is also of high quality, being grammatically correct, concise, and using appropriate technical terminology."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with human preference for a technical explanation. The content is safe and helpful. The diagram is aesthetically pleasing, clear, and easy to understand."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both a figure and a separate text caption as requested. The figure successfully illustrates all three required components: (a) VPT structure, (b) 2D FFT over prompts, and (c) VFPT integration."
        ],
        "Content Richness": [
          5,
          "The response is very rich. The diagram provides a clear visual breakdown of a complex process. The text caption is more detailed and explanatory than the reference caption, offering a better understanding of what each component does, which is highly valuable."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"We propose a lightweight visual prompt tuning method, called Visual Fourier Prompt Tuning (VFPT), to improve efficiency and expressiveness in transformer-based visual models. VFPT introduces 2D Fast Fourier Transform (FFT) into partial visual prompts along both hidden and sequence dimensions. The design is compared with original Visual Prompt Tuning (VPT).\\nPlease provide:\\n 1. A clear figure illustrating: (a) VPT structure, (b) 2D FFT over prompts, (c) VFPT integration into transformer encoder.\\n 2. A concise figure caption. Output the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both factually correct. The diagrams accurately represent the concepts of Visual Prompt Tuning (VPT), the 2D Fast Fourier Transform (FFT) process on prompts, and the integration of the resulting Visual Fourier Prompt Tuning (VFPT) prompts into a transformer encoder, consistent with the reference.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text caption perfectly aligns with the generated image. Each part of the caption, (a), (b), and (c), corresponds directly and accurately to the respective sections of the diagram, providing a clear explanation of the visual elements.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The image quality is excellent for a technical diagram. It is clear, well-organized, and uses a consistent and professional style. The text is also of high quality, being grammatically correct, concise, and using appropriate technical terminology.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with human preference for a technical explanation. The content is safe and helpful. The diagram is aesthetically pleasing, clear, and easy to understand.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both a figure and a separate text caption as requested. The figure successfully illustrates all three required components: (a) VPT structure, (b) 2D FFT over prompts, and (c) VFPT integration.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The response is very rich. The diagram provides a clear visual breakdown of a complex process. The text caption is more detailed and explanatory than the reference caption, offering a better understanding of what each component does, which is highly valuable.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 34,
    "domain": "diagram",
    "question": "I am preparing a figure to illustrate different pipelines for video anomaly detection (VAD) using vision-language models (VLMs). The figure should compare three approaches:\n\n1. **Pipeline 1**: A frozen VLM is used for video description and relies on an external LLM for reasoning. The final decision (normal or abnormal) is made using coarse-labeled training data.\n2. **Pipeline 2**: Instruction tuning is applied to the VLM, allowing it to describe and reason jointly. This approach requires training on instruction datasets with frame-level annotations.\n3. **VERA (our method)**: Introduces VAD guiding questions to query a frozen VLM, enabling it to describe and reason without instruction tuning. The guiding questions are learnable and derived from coarsely labeled data.\n\nIn the figure, visually distinguish between frozen modules, learnable modules, inference, and training steps. Show side-by-side comparisons for these three methods, and use example video frames to indicate both test input and training sets.\n\nPlease generate:\n 1. A comparative diagram of the three VAD pipelines with appropriate arrows, color codes, and labels.\n 2. A separate caption summarizing the figure. Provide the image and caption separately.",
    "reference_text": "Figure 1. VERA renders frozen VLMs to describe and reason with learnable guiding questions learned from coarsely labeled data.",
    "reference_images": [
      "data/diagram/VERA_VAD_Pipelines.png"
    ],
    "generated_text": "# Figure Caption (separate)\n\n**Figure X: Side-by-side comparison of three VAD pipelines using VLMs.**\n**(Left) Pipeline 1 — Frozen VLM + External LLM:** test video frames are captioned by a **frozen VLM** (blue); an **external LLM** performs reasoning, and a small **decision head** is trained only with **coarse-labeled videos** (dashed training arrows) to output *Normal/Abnormal*.\n**(Middle) Pipeline 2 — Instruction-Tuned VLM:** the VLM becomes **learnable** (orange) and is **instruction-tuned** to *describe + reason* jointly using **instruction datasets with frame-level annotations** (green). The tuned VLM feeds a decision head at inference (solid arrows).\n**(Right) VERA — Our Method:** **learnable VAD guiding questions** are trained from **coarse labels** and used to **query a frozen VLM**, eliciting descriptions and reasoning *without instruction tuning*; only the question module and decision head are learned.\n**Legend:** Frozen modules = blue, learnable modules = orange, datasets = green, **solid** arrows = inference, **dashed** arrows = training.\n",
    "generated_image": "GPT-thinking/diagram/34.png",
    "gemini_evaluation": {
      "Question": "I am preparing a figure to illustrate different pipelines for video anomaly detection (VAD) using vision-language models (VLMs). The figure should compare three approaches:\n\n1. **Pipeline 1**: A frozen VLM is used for video description and relies on an external LLM for reasoning. The final decision (normal or abnormal) is made using coarse-labeled training data.\n2. **Pipeline 2**: Instruction tuning is applied to the VLM, allowing it to describe and reason jointly. This approach requires training on instruction datasets with frame-level annotations.\n3. **VERA (our method)**: Introduces VAD guiding questions to query a frozen VLM, enabling it to describe and reason without instruction tuning. The guiding questions are learnable and derived from coarsely labeled data.\n\nIn the figure, visually distinguish between frozen modules, learnable modules, inference, and training steps. Show side-by-side comparisons for these three methods, and use example video frames to indicate both test input and training sets.\n\nPlease generate:\n 1. A comparative diagram of the three VAD pipelines with appropriate arrows, color codes, and labels.\n 2. A separate caption summarizing the figure. Provide the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both entirely correct. The diagram accurately represents all three specified pipelines, correctly distinguishing between frozen/learnable modules, training/inference steps, and data sources as requested in the prompt. The text caption is a precise and factual description of the diagram."
        ],
        "Image-Text Coherency": [
          5,
          "There is perfect coherence between the generated image and text. The text caption explicitly describes the visual elements of the diagram, including the color-coding (blue for frozen, orange for learnable), arrow styles (solid for inference, dashed for training), and the components of each of the three pipelines. Every detail mentioned in the caption is accurately depicted in the image."
        ],
        "Content Quality": [
          5,
          "The content quality is excellent. The diagram is clear, professionally laid out, and easy to interpret, effectively using visual aids like color and icons to convey complex information. The text is well-written, grammatically correct, and structured logically with a clear title, per-pipeline descriptions, and a legend."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with human preference for a scientific or technical illustration. It is clear, accurate, and professional. There is no undesirable or inappropriate content."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both the requested comparative diagram and the separate text caption, fulfilling both parts of the prompt. All specified components and details are included."
        ],
        "Content Richness": [
          5,
          "The response is very rich in detail. The diagram includes small visual examples for the video inputs/datasets, which adds clarity. The text is detailed and comprehensive, breaking down the explanation for each pipeline and providing a full legend, making the figure self-contained and easy to understand. It successfully captures the same level of detail as the reference."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I am preparing a figure to illustrate different pipelines for video anomaly detection (VAD) using vision-language models (VLMs). The figure should compare three approaches:\\n\\n1. **Pipeline 1**: A frozen VLM is used for video description and relies on an external LLM for reasoning. The final decision (normal or abnormal) is made using coarse-labeled training data.\\n2. **Pipeline 2**: Instruction tuning is applied to the VLM, allowing it to describe and reason jointly. This approach requires training on instruction datasets with frame-level annotations.\\n3. **VERA (our method)**: Introduces VAD guiding questions to query a frozen VLM, enabling it to describe and reason without instruction tuning. The guiding questions are learnable and derived from coarsely labeled data.\\n\\nIn the figure, visually distinguish between frozen modules, learnable modules, inference, and training steps. Show side-by-side comparisons for these three methods, and use example video frames to indicate both test input and training sets.\\n\\nPlease generate:\\n 1. A comparative diagram of the three VAD pipelines with appropriate arrows, color codes, and labels.\\n 2. A separate caption summarizing the figure. Provide the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both entirely correct. The diagram accurately represents all three specified pipelines, correctly distinguishing between frozen/learnable modules, training/inference steps, and data sources as requested in the prompt. The text caption is a precise and factual description of the diagram.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"There is perfect coherence between the generated image and text. The text caption explicitly describes the visual elements of the diagram, including the color-coding (blue for frozen, orange for learnable), arrow styles (solid for inference, dashed for training), and the components of each of the three pipelines. Every detail mentioned in the caption is accurately depicted in the image.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The content quality is excellent. The diagram is clear, professionally laid out, and easy to interpret, effectively using visual aids like color and icons to convey complex information. The text is well-written, grammatically correct, and structured logically with a clear title, per-pipeline descriptions, and a legend.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with human preference for a scientific or technical illustration. It is clear, accurate, and professional. There is no undesirable or inappropriate content.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both the requested comparative diagram and the separate text caption, fulfilling both parts of the prompt. All specified components and details are included.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The response is very rich in detail. The diagram includes small visual examples for the video inputs/datasets, which adds clarity. The text is detailed and comprehensive, breaking down the explanation for each pipeline and providing a full legend, making the figure self-contained and easy to understand. It successfully captures the same level of detail as the reference.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 104,
    "domain": "diagram",
    "question": "I'm preparing a figure for a simulation framework named BookWorld. The framework simulates interactions between story characters extracted from books. It involves three stages: (1) Initialization, where characters and world information are extracted from books; (2) Simulation, where characters interact in dynamically evolving scenes with memories and goals; and (3) Rephrasing, where interaction logs are compiled into a narrative. Please visualize the entire pipeline: agent initialization, dynamic attribute updates, scene-wise interactions, and the transformation into novel-style text. Also generate:\n\n1. A visual diagram showing this BookWorld framework.\n2. A short, publication-style caption summarizing the visual.\nPlease output the diagram path and caption separately.",
    "reference_text": "Figure 2: The complete pipeline of BookWorld simulation. The framework starts from initializing the agents, then the agents interact across scenes.",
    "reference_images": [
      "data/diagram/BookWorld-Framework.png"
    ],
    "generated_text": "BookWorld pipeline: (1) Initialization extracts characters and world knowledge from books to instantiate agents with traits, memories, goals, and a shared world graph; (2) a scene-wise multi-agent simulation runs perception→retrieval→planning→action loops, logging events while dynamically updating attributes (memories, relations, goals, state); (3) a rephrasing stage compiles the logs into coherent novel-style narrative text.",
    "generated_image": "GPT-thinking/diagram/104.png",
    "gemini_evaluation": {
      "Question": "I'm preparing a figure for a simulation framework named BookWorld. The framework simulates interactions between story characters extracted from books. It involves three stages: (1) Initialization, where characters and world information are extracted from books; (2) Simulation, where characters interact in dynamically evolving scenes with memories and goals; and (3) Rephrasing, where interaction logs are compiled into a narrative. Please visualize the entire pipeline: agent initialization, dynamic attribute updates, scene-wise interactions, and the transformation into novel-style text. Also generate:\n\n1. A visual diagram showing this BookWorld framework.\n2. A short, publication-style caption summarizing the visual.\nPlease output the diagram path and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text correctly represent the three-stage pipeline (Initialization, Simulation, Rephrasing) described in the prompt. All key components requested, such as agent initialization, dynamic updates, and transformation to text, are accurately included."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text serves as a perfect caption for the generated image. Each of the three numbered points in the text directly corresponds to one of the three main sections of the visual diagram, creating a strong and clear connection between the two."
        ],
        "Content Quality": [
          5,
          "The generated image is a high-quality, clear, and professional-looking diagram suitable for a publication. The text is well-written, concise, and grammatically correct, effectively summarizing the process in a formal style."
        ],
        "Human Preference Alignment": [
          5,
          "The output is aesthetically pleasing and directly addresses the user's request for a publication-style figure. There is no undesirable or inappropriate content."
        ],
        "Completeness": [
          5,
          "The response successfully provides both the visual diagram and the text caption as requested in the prompt's two requirements."
        ],
        "Content Richness": [
          5,
          "The generated response is richer than the reference. The diagram includes more specific details about the agent's action loop (perception, retrieval, etc.), and the text caption is far more descriptive and informative than the very brief reference text, providing a comprehensive overview of the entire pipeline."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm preparing a figure for a simulation framework named BookWorld. The framework simulates interactions between story characters extracted from books. It involves three stages: (1) Initialization, where characters and world information are extracted from books; (2) Simulation, where characters interact in dynamically evolving scenes with memories and goals; and (3) Rephrasing, where interaction logs are compiled into a narrative. Please visualize the entire pipeline: agent initialization, dynamic attribute updates, scene-wise interactions, and the transformation into novel-style text. Also generate:\\n\\n1. A visual diagram showing this BookWorld framework.\\n2. A short, publication-style caption summarizing the visual.\\nPlease output the diagram path and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text correctly represent the three-stage pipeline (Initialization, Simulation, Rephrasing) described in the prompt. All key components requested, such as agent initialization, dynamic updates, and transformation to text, are accurately included.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text serves as a perfect caption for the generated image. Each of the three numbered points in the text directly corresponds to one of the three main sections of the visual diagram, creating a strong and clear connection between the two.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The generated image is a high-quality, clear, and professional-looking diagram suitable for a publication. The text is well-written, concise, and grammatically correct, effectively summarizing the process in a formal style.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The output is aesthetically pleasing and directly addresses the user's request for a publication-style figure. There is no undesirable or inappropriate content.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response successfully provides both the visual diagram and the text caption as requested in the prompt's two requirements.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The generated response is richer than the reference. The diagram includes more specific details about the agent's action loop (perception, retrieval, etc.), and the text caption is far more descriptive and informative than the very brief reference text, providing a comprehensive overview of the entire pipeline.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 6,
    "domain": "diagram",
    "question": "I am preparing a figure for my paper to illustrate the following framework:\nDiagramAgent is a multi-agent framework proposed for text-to-diagram generation and editing. It decomposes the process into modular stages that simulate how humans might approach diagram construction. The framework handles three types of tasks: Diagram Generation: A user provides a natural language instruction describing a diagram to be created from scratch. Diagram Coding: The system reverse-engineers structured code from a given diagram image. Diagram Editing: A user modifies an existing diagram by providing revision instructions. The process begins when the Plan Agent receives a user query. It parses the instruction and expands it into a more explicit query detailing the required visual components (e.g., input/output nodes, layer types, layout constraints). This planning phase utilizes an LLM to assign sub-tasks. The expanded query is routed to either the Code Agent, which generates diagram code from the detailed instruction, or to the Diagram-to-Code Agent, which reconstructs code from visual inputs. The Check Agent performs code verification, debugging, and feedback. The agent ensures the output is valid and interpretable, and facilitates iteration when errors occur. The entire system supports end-to-end generation from instruction to rendered diagram, including code-based feedback and visual output comparison.\nPlease generate:\n\t1.\tA visual diagram showing how these components interact.\n\t2.\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.",
    "reference_text": "DiagramAgent architecture for structured visual generation. The framework supports diagram creation, coding, and editing by coordinating a Plan Agent (task decomposition), Code and Diagram-to-Code Agents (code synthesis or recovery), and a Check Agent (verification and debugging). Together, the agents enable an iterative, controllable pipeline from textual instructions to structured diagrams.",
    "reference_images": [
      "data/diagram/2411.11916v1.png"
    ],
    "generated_text": "DiagramAgent is a modular, multi-agent pipeline for text-to-diagram generation and editing: a Plan Agent expands user intent into explicit visual requirements and routes work either to a Code Agent (produce diagram code from text) or a Diagram-to-Code Agent (reconstruct code from an image). A Check Agent validates and debugs the code, compares rendered outputs against the specification, and drives an iterative feedback loop—supporting end-to-end Diagram Generation, Diagram Coding, and Diagram Editing.",
    "generated_image": "GPT-thinking/diagram/6.png",
    "gemini_evaluation": {
      "Question": "I am preparing a figure for my paper to illustrate the following framework:\nDiagramAgent is a multi-agent framework proposed for text-to-diagram generation and editing. It decomposes the process into modular stages that simulate how humans might approach diagram construction. The framework handles three types of tasks: Diagram Generation: A user provides a natural language instruction describing a diagram to be created from scratch. Diagram Coding: The system reverse-engineers structured code from a given diagram image. Diagram Editing: A user modifies an existing diagram by providing revision instructions. The process begins when the Plan Agent receives a user query. It parses the instruction and expands it into a more explicit query detailing the required visual components (e.g., input/output nodes, layer types, layout constraints). This planning phase utilizes an LLM to assign sub-tasks. The expanded query is routed to either the Code Agent, which generates diagram code from the detailed instruction, or to the Diagram-to-Code Agent, which reconstructs code from visual inputs. The Check Agent performs code verification, debugging, and feedback. The agent ensures the output is valid and interpretable, and facilitates iteration when errors occur. The entire system supports end-to-end generation from instruction to rendered diagram, including code-based feedback and visual output comparison.\nPlease generate:\n\t1.\tA visual diagram showing how these components interact.\n\t2.\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both factually correct and accurately represent the framework described in the user's prompt. All agents, tasks, and their interactions are correctly depicted and described."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text serves as a perfect caption for the generated image. Every component mentioned in the text (Plan Agent, Code Agent, etc.) is clearly visualized in the diagram, and the flow described in the text matches the arrows and structure of the image."
        ],
        "Content Quality": [
          5,
          "The image is of high quality: it is clear, well-organized, and uses a professional and easy-to-read style suitable for a publication. The text is well-written, concise, and grammatically flawless."
        ],
        "Human Preference Alignment": [
          5,
          "The response is excellent. The diagram is aesthetically pleasing and professional, and the text is clear and informative. It fully meets the user's request without any undesirable content."
        ],
        "Completeness": [
          5,
          "The response is complete, providing both of the requested outputs: a visual diagram and a separate text caption. All key elements from the prompt are included."
        ],
        "Content Richness": [
          4,
          "The response is quite rich and detailed, successfully capturing the complexity of the framework. The diagram includes the main agents, their functions, and the overall workflow. It could be slightly richer by explicitly visualizing the different *inputs* for each task (e.g., text vs. image input), as the reference image does, but this is a minor omission."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I am preparing a figure for my paper to illustrate the following framework:\\nDiagramAgent is a multi-agent framework proposed for text-to-diagram generation and editing. It decomposes the process into modular stages that simulate how humans might approach diagram construction. The framework handles three types of tasks: Diagram Generation: A user provides a natural language instruction describing a diagram to be created from scratch. Diagram Coding: The system reverse-engineers structured code from a given diagram image. Diagram Editing: A user modifies an existing diagram by providing revision instructions. The process begins when the Plan Agent receives a user query. It parses the instruction and expands it into a more explicit query detailing the required visual components (e.g., input/output nodes, layer types, layout constraints). This planning phase utilizes an LLM to assign sub-tasks. The expanded query is routed to either the Code Agent, which generates diagram code from the detailed instruction, or to the Diagram-to-Code Agent, which reconstructs code from visual inputs. The Check Agent performs code verification, debugging, and feedback. The agent ensures the output is valid and interpretable, and facilitates iteration when errors occur. The entire system supports end-to-end generation from instruction to rendered diagram, including code-based feedback and visual output comparison.\\nPlease generate:\\n\\t1.\\tA visual diagram showing how these components interact.\\n\\t2.\\tA separate caption summarizing the key idea of the figure. Please provide the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both factually correct and accurately represent the framework described in the user's prompt. All agents, tasks, and their interactions are correctly depicted and described.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text serves as a perfect caption for the generated image. Every component mentioned in the text (Plan Agent, Code Agent, etc.) is clearly visualized in the diagram, and the flow described in the text matches the arrows and structure of the image.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The image is of high quality: it is clear, well-organized, and uses a professional and easy-to-read style suitable for a publication. The text is well-written, concise, and grammatically flawless.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is excellent. The diagram is aesthetically pleasing and professional, and the text is clear and informative. It fully meets the user's request without any undesirable content.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is complete, providing both of the requested outputs: a visual diagram and a separate text caption. All key elements from the prompt are included.\"\n        ],\n        \"Content Richness\": [\n            4,\n            \"The response is quite rich and detailed, successfully capturing the complexity of the framework. The diagram includes the main agents, their functions, and the overall workflow. It could be slightly richer by explicitly visualizing the different *inputs* for each task (e.g., text vs. image input), as the reference image does, but this is a minor omission.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 94,
    "domain": "diagram",
    "question": "I'm working on visual reasoning dataset construction and want to visualize a data pipeline that handles both cold-start and training data. My method includes:\n\n1. A **Cold-start Reasoning Data** pipeline using distilled text-only reasoning data to train annotators for visual reasoning understanding.\n2. A **Visual Reasoning Annotator** that takes raw instruction data (image, question, answer) and generates visual reasoning examples.\n3. A **Reject Sampling & Rewriting** module for verifying and refining outputs (format, correctness, visual grounding, answer).\n4. The pipeline outputs verified **Visual Reasoning Training Data**, which are high-quality, grounded examples suitable for training VQA models.\n\nPlease design:\n 1. A figure that clearly illustrates the data construction framework, with both cold-start (annotator training) and visual reasoning training pipelines.\n 2. A short, publication-ready caption for the figure. Output image path and caption separately.",
    "reference_text": "Figure 2: Overview framework of our data pipeline. The blue arrow line indicates the cold-start data curation pipeline for the annotator and the green line indicates the data pipeline for training data.",
    "reference_images": [
      "data/diagram/Visual-Reasoning-Data-Pipeline.png"
    ],
    "generated_text": "Data construction framework with two coordinated tracks. Left (cold-start): distilled text-only reasoning corpora → curriculum/examples → annotator training to instill visual-reasoning formats and schemas. Right (training): the trained Visual Reasoning Annotator converts raw instruction data (image, question, answer) into candidate reasoning examples. A Reject Sampling & Rewriting gate checks format, correctness, visual grounding, and final answer; accepted items become verified Visual Reasoning Training Data for VQA model training.",
    "generated_image": "GPT-thinking/diagram/94.png",
    "gemini_evaluation": {
      "Question": "I'm working on visual reasoning dataset construction and want to visualize a data pipeline that handles both cold-start and training data. My method includes:\n\n1. A **Cold-start Reasoning Data** pipeline using distilled text-only reasoning data to train annotators for visual reasoning understanding.\n2. A **Visual Reasoning Annotator** that takes raw instruction data (image, question, answer) and generates visual reasoning examples.\n3. A **Reject Sampling & Rewriting** module for verifying and refining outputs (format, correctness, visual grounding, answer).\n4. The pipeline outputs verified **Visual Reasoning Training Data**, which are high-quality, grounded examples suitable for training VQA models.\n\nPlease design:\n 1. A figure that clearly illustrates the data construction framework, with both cold-start (annotator training) and visual reasoning training pipelines.\n 2. A short, publication-ready caption for the figure. Output image path and caption separately.",
      "Evaluation": {
        "Correctness": [
          4,
          "The generated image and text correctly represent the two main pipelines described in the prompt. However, the generated image fails to explicitly connect the output of the 'Annotator Training' (the trained annotator) to the 'Visual Reasoning Annotator' module in the main pipeline. The reference image does a better job of showing this connection, making the overall flow clearer."
        ],
        "Image-Text Coherency": [
          4,
          "The text and image are mostly coherent. The text describes a 'trained Visual Reasoning Annotator', which implies the connection from the cold-start pipeline. The image, however, does not visually represent this connection, creating a slight disconnect. The text essentially has to fill in a logical gap left by the diagram."
        ],
        "Content Quality": [
          5,
          "The generated image is of high quality, with a clean, professional design, clear labels, and effective use of icons and color to distinguish the two pipelines. The text is also high-quality, being well-written, concise, and perfectly suited for a publication caption. It is significantly more descriptive and polished than the reference text."
        ],
        "Human Preference Alignment": [
          5,
          "The response is excellent from a human preference standpoint. The diagram is aesthetically pleasing and easy to interpret, and the text is formal and informative. The content is perfectly appropriate for the requested academic/technical context."
        ],
        "Completeness": [
          4,
          "The response provides both the requested image and the caption. It's slightly incomplete because the diagram misses the crucial visual link between the two pipelines, which is a key part of the overall framework described in the prompt. The reference image is more complete in this regard."
        ],
        "Content Richness": [
          5,
          "The generated response is very rich. The image uses icons and a more modern design than the simple boxes of the reference. The text caption is far more detailed and descriptive than the reference, providing a comprehensive overview of the entire process, which adds significant value and clarity."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm working on visual reasoning dataset construction and want to visualize a data pipeline that handles both cold-start and training data. My method includes:\\n\\n1. A **Cold-start Reasoning Data** pipeline using distilled text-only reasoning data to train annotators for visual reasoning understanding.\\n2. A **Visual Reasoning Annotator** that takes raw instruction data (image, question, answer) and generates visual reasoning examples.\\n3. A **Reject Sampling & Rewriting** module for verifying and refining outputs (format, correctness, visual grounding, answer).\\n4. The pipeline outputs verified **Visual Reasoning Training Data**, which are high-quality, grounded examples suitable for training VQA models.\\n\\nPlease design:\\n 1. A figure that clearly illustrates the data construction framework, with both cold-start (annotator training) and visual reasoning training pipelines.\\n 2. A short, publication-ready caption for the figure. Output image path and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            4,\n            \"The generated image and text correctly represent the two main pipelines described in the prompt. However, the generated image fails to explicitly connect the output of the 'Annotator Training' (the trained annotator) to the 'Visual Reasoning Annotator' module in the main pipeline. The reference image does a better job of showing this connection, making the overall flow clearer.\"\n        ],\n        \"Image-Text Coherency\": [\n            4,\n            \"The text and image are mostly coherent. The text describes a 'trained Visual Reasoning Annotator', which implies the connection from the cold-start pipeline. The image, however, does not visually represent this connection, creating a slight disconnect. The text essentially has to fill in a logical gap left by the diagram.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The generated image is of high quality, with a clean, professional design, clear labels, and effective use of icons and color to distinguish the two pipelines. The text is also high-quality, being well-written, concise, and perfectly suited for a publication caption. It is significantly more descriptive and polished than the reference text.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is excellent from a human preference standpoint. The diagram is aesthetically pleasing and easy to interpret, and the text is formal and informative. The content is perfectly appropriate for the requested academic/technical context.\"\n        ],\n        \"Completeness\": [\n            4,\n            \"The response provides both the requested image and the caption. It's slightly incomplete because the diagram misses the crucial visual link between the two pipelines, which is a key part of the overall framework described in the prompt. The reference image is more complete in this regard.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The generated response is very rich. The image uses icons and a more modern design than the simple boxes of the reference. The text caption is far more detailed and descriptive than the reference, providing a comprehensive overview of the entire process, which adds significant value and clarity.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 59,
    "domain": "diagram",
    "question": "I'm creating a figure to illustrate the GraphStitch unit architecture in UniGAD.\nThe figure has two layers:\n1. **Multi-level Layer1**: Processes node, edge, and graph features separately but identically. Each feature level is connected to a **GraphStitch Unit**, which computes interaction attention weights across levels (e.g., α<sub>nn</sub>, α<sub>ne</sub>, α<sub>ng</sub> for node level).\n2. **Multi-level Layer2**: Receives processed outputs from Layer1, again uses a **GraphStitch Unit** with updated attention weights for cross-level feature fusion. The architecture is repeated across layers with the node-level highlighted.\nPlease provide:\n 1. A clear figure illustrating the GraphStitch unit and multi-level interaction structure.\n 2. A concise figure caption. Output the image and caption separately.",
    "reference_text": "Figure 4: GraphStitch network structure in UniGAD. Node level is highlighted.",
    "reference_images": [
      "data/diagram/UniGAD-GraphStitch.png"
    ],
    "generated_text": "**Figure caption:**GraphStitch unit architecture for UniGAD. Each multi-level layer (Layer1 → Layer2) processes node, edge, and graph features in parallel with identical level-specific processors, then fuses them via a GraphStitch Unit that learns cross-level interaction attention weights $\\alpha_{\\ell m}$ (illustrated for the node stream as $\\alpha_{nn}, \\alpha_{ne}, \\alpha_{ng}$). Layer2 consumes the outputs of Layer1 and applies an updated attention matrix $\\alpha'_{\\ell m}$; the pattern repeats across layers, with the node pathway highlighted.",
    "generated_image": "GPT-thinking/diagram/59.png",
    "gemini_evaluation": {
      "Question": "I'm creating a figure to illustrate the GraphStitch unit architecture in UniGAD.\nThe figure has two layers:\n1. **Multi-level Layer1**: Processes node, edge, and graph features separately but identically. Each feature level is connected to a **GraphStitch Unit**, which computes interaction attention weights across levels (e.g., α<sub>nn</sub>, α<sub>ne</sub>, α<sub>ng</sub> for node level).\n2. **Multi-level Layer2**: Receives processed outputs from Layer1, again uses a **GraphStitch Unit** with updated attention weights for cross-level feature fusion. The architecture is repeated across layers with the node-level highlighted.\nPlease provide:\n 1. A clear figure illustrating the GraphStitch unit and multi-level interaction structure.\n 2. A concise figure caption. Output the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both correct. The image accurately visualizes the two-layer architecture with parallel processing streams for node, edge, and graph features, the identical processors, and the GraphStitch units with the specified attention weights. The text correctly and precisely describes all the components and processes shown in the diagram."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text perfectly describes the generated image. Every element mentioned in the caption—the two layers, the parallel feature streams, the identical processors, the GraphStitch units, the specific attention weights (α and α'), and the highlighted node pathway—is clearly and accurately depicted in the visual diagram. The alignment is flawless."
        ],
        "Content Quality": [
          5,
          "The quality is excellent. The image is a clear, professional-grade technical diagram with a logical layout, legible text, and effective use of color to highlight the node pathway. The text is well-written, grammatically correct, and uses appropriate technical language and LaTeX formatting for symbols, making it a high-quality, descriptive caption."
        ],
        "Human Preference Alignment": [
          5,
          "The response is highly aligned with human preference. It provides a clean, aesthetically pleasing, and easy-to-understand diagram for a complex architecture. The caption is informative and helpful. The entire output is professional and directly addresses the user's request without any issues."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both of the requested items: a clear figure and a separate, concise caption. All the specific architectural details mentioned in the prompt are included in the figure and explained in the text."
        ],
        "Content Richness": [
          5,
          "The response is very rich. The diagram is detailed, showing the flow of data and the specific mathematical notations for the attention weights. The caption is significantly more descriptive and informative than the reference, providing a comprehensive explanation of the entire architecture, which adds substantial value for the reader."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm creating a figure to illustrate the GraphStitch unit architecture in UniGAD.\\nThe figure has two layers:\\n1. **Multi-level Layer1**: Processes node, edge, and graph features separately but identically. Each feature level is connected to a **GraphStitch Unit**, which computes interaction attention weights across levels (e.g., α<sub>nn</sub>, α<sub>ne</sub>, α<sub>ng</sub> for node level).\\n2. **Multi-level Layer2**: Receives processed outputs from Layer1, again uses a **GraphStitch Unit** with updated attention weights for cross-level feature fusion. The architecture is repeated across layers with the node-level highlighted.\\nPlease provide:\\n 1. A clear figure illustrating the GraphStitch unit and multi-level interaction structure.\\n 2. A concise figure caption. Output the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both correct. The image accurately visualizes the two-layer architecture with parallel processing streams for node, edge, and graph features, the identical processors, and the GraphStitch units with the specified attention weights. The text correctly and precisely describes all the components and processes shown in the diagram.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text perfectly describes the generated image. Every element mentioned in the caption—the two layers, the parallel feature streams, the identical processors, the GraphStitch units, the specific attention weights (α and α'), and the highlighted node pathway—is clearly and accurately depicted in the visual diagram. The alignment is flawless.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The quality is excellent. The image is a clear, professional-grade technical diagram with a logical layout, legible text, and effective use of color to highlight the node pathway. The text is well-written, grammatically correct, and uses appropriate technical language and LaTeX formatting for symbols, making it a high-quality, descriptive caption.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is highly aligned with human preference. It provides a clean, aesthetically pleasing, and easy-to-understand diagram for a complex architecture. The caption is informative and helpful. The entire output is professional and directly addresses the user's request without any issues.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both of the requested items: a clear figure and a separate, concise caption. All the specific architectural details mentioned in the prompt are included in the figure and explained in the text.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The response is very rich. The diagram is detailed, showing the flow of data and the specific mathematical notations for the attention weights. The caption is significantly more descriptive and informative than the reference, providing a comprehensive explanation of the entire architecture, which adds substantial value for the reader.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 69,
    "domain": "diagram",
    "question": "I'm creating a figure to illustrate the architecture of PathWeave, a continual modality learning framework for vision-language models.\nThe figure has two parts:\n1. **Architecture Overview (Left)**: This shows the PathWeave system beginning with multiple modality encoders (image, video, audio, depth, point), feeding into a frozen LLM via a Q-Former with Adapter-in-Adapter (AnA) modules. It includes modality expansion using adapters for cross-modal alignment.\n2. **Training vs. Testing (Right)**: The right part highlights the difference between training and testing on different modalities using uni-modal adapters and a gating mechanism (MoE-based $G^m$).\nPlease provide:\n 1. A clear figure visualizing the PathWeave architecture and modality expansion method.\n 2. A concise figure caption. Output the image and caption separately.",
    "reference_text": "Figure 2: PathWeave continually expands modality support using frozen LLMs and adapters. Left: modality encoders feed into Q-Former with AnA for cross-modal alignment. Right: Training (bottom) uses trainable adapters and gating; testing (top) uses frozen ones for inference.",
    "reference_images": [
      "data/diagram/PathWeave-ContinualModalLearning.png"
    ],
    "generated_text": "# Figure Caption — PathWeave\n\n**PathWeave: Continual Modality Learning for VLMs.**\n**(a) Architecture Overview (left):** Image, video, audio, depth, and point encoders feed visual/audio tokens into a **Q-Former** equipped with **Adapter-in-Adapter (AnA)** blocks. **Expansion adapters** enable adding new modalities without touching the backbone; AnA aligns cross-modal features before passing them to a **frozen LLM (❄️)** for reasoning. Trainable modules are marked with 🔥; adapters are highlighted and grouped to show cross-modal alignment and expansion.\n**(b) Training vs. Testing (right):** During **training**, modality-specific tokens pass through **uni-modal adapters** and a **MoE-based gate $G^m$** that routes/weights experts to learn per-modality paths. During **testing**, the same gate selects the appropriate uni-modal adapter(s) for the incoming modality, producing routed tokens for the frozen LLM. Legends distinguish frozen vs. trainable, adapter blocks, and the gating unit.\n",
    "generated_image": "GPT-thinking/diagram/69.png",
    "gemini_evaluation": {
      "Question": "I'm creating a figure to illustrate the architecture of PathWeave, a continual modality learning framework for vision-language models.\nThe figure has two parts:\n1. **Architecture Overview (Left)**: This shows the PathWeave system beginning with multiple modality encoders (image, video, audio, depth, point), feeding into a frozen LLM via a Q-Former with Adapter-in-Adapter (AnA) modules. It includes modality expansion using adapters for cross-modal alignment.\n2. **Training vs. Testing (Right)**: The right part highlights the difference between training and testing on different modalities using uni-modal adapters and a gating mechanism (MoE-based $G^m$).\nPlease provide:\n 1. A clear figure visualizing the PathWeave architecture and modality expansion method.\n 2. A concise figure caption. Output the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text correctly represent all the components of the PathWeave architecture as described in the prompt and the reference. All key elements like multiple encoders, Q-Former, AnA, frozen LLM, and the training/testing distinction with uni-modal adapters and gating are accurately included."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text is an excellent and detailed description of the generated image. It is structured into parts (a) and (b) that directly correspond to the left and right panels of the figure, and it even explains the iconography (🔥, ❄️) and legend used in the image. The alignment is perfect."
        ],
        "Content Quality": [
          4,
          "The text quality is excellent—it is well-structured, clear, and grammatically correct. The image is conceptually correct and contains all the necessary information, but its visual execution is slightly cluttered and less polished than the reference. Some text labels within the image are small and slightly distorted, which slightly reduces the overall quality."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with human preferences. The content is safe, relevant, and presents the technical information in a helpful and appropriate manner. The diagram's style is suitable for a technical illustration."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both the requested image and a separate, detailed text caption, fulfilling all aspects of the user's prompt."
        ],
        "Content Richness": [
          5,
          "The generated content is very rich. The image uses color-coding, icons, and a legend to convey a lot of information effectively. The text caption is significantly more detailed and explanatory than the concise reference caption, providing a much deeper understanding of the architecture, which is a major strength."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I'm creating a figure to illustrate the architecture of PathWeave, a continual modality learning framework for vision-language models.\\nThe figure has two parts:\\n1. **Architecture Overview (Left)**: This shows the PathWeave system beginning with multiple modality encoders (image, video, audio, depth, point), feeding into a frozen LLM via a Q-Former with Adapter-in-Adapter (AnA) modules. It includes modality expansion using adapters for cross-modal alignment.\\n2. **Training vs. Testing (Right)**: The right part highlights the difference between training and testing on different modalities using uni-modal adapters and a gating mechanism (MoE-based $G^m$).\\nPlease provide:\\n 1. A clear figure visualizing the PathWeave architecture and modality expansion method.\\n 2. A concise figure caption. Output the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text correctly represent all the components of the PathWeave architecture as described in the prompt and the reference. All key elements like multiple encoders, Q-Former, AnA, frozen LLM, and the training/testing distinction with uni-modal adapters and gating are accurately included.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text is an excellent and detailed description of the generated image. It is structured into parts (a) and (b) that directly correspond to the left and right panels of the figure, and it even explains the iconography (🔥, ❄️) and legend used in the image. The alignment is perfect.\"\n        ],\n        \"Content Quality\": [\n            4,\n            \"The text quality is excellent—it is well-structured, clear, and grammatically correct. The image is conceptually correct and contains all the necessary information, but its visual execution is slightly cluttered and less polished than the reference. Some text labels within the image are small and slightly distorted, which slightly reduces the overall quality.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with human preferences. The content is safe, relevant, and presents the technical information in a helpful and appropriate manner. The diagram's style is suitable for a technical illustration.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both the requested image and a separate, detailed text caption, fulfilling all aspects of the user's prompt.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The generated content is very rich. The image uses color-coding, icons, and a legend to convey a lot of information effectively. The text caption is significantly more detailed and explanatory than the concise reference caption, providing a much deeper understanding of the architecture, which is a major strength.\"\n        ]\n    }\n}\n```"
  },
  {
    "id": 16,
    "domain": "diagram",
    "question": "I am preparing a figure for my paper to illustrate the following framework:\nThis diagram presents the pipeline of the proposed **StyleMaster**, which performs video stylization guided by a style image and a textual description.\n\nThe framework is composed of the following major components:\n\n1. **Input & Output**:\n   - The input is a raw video and a style image (e.g., Van Gogh painting).\n   - The output is a stylized video with artistic consistency.\n\n2. **Style Extraction**:\n   - The style image is encoded using a **CLIP image encoder** to extract patch-level features and image embeddings.\n   - Patches with lower similarity to the accompanying text prompt (e.g., \"A lone penguin walks on a sandy beach at sunset.\") are selected for style representation.\n   - A local selection module (Q-Former) refines these patches.\n   - A **global projection module** transforms them into global style tokens using contrastive learning, enhanced with specially constructed data pairs.\n\n3. **Contrastive Dataset Construction**:\n   - Positive pairs are created by rearranging anchor images.\n   - Negative samples are synthesized via **model illusion** from arbitrary text-to-image models.\n\n4. **Stylization Network (DiT Block)**:\n   - A spatial-temporal diffusion transformer (DiT) applies multiple blocks of self-attention and cross-attention.\n   - Style and text features are injected via decoupled cross-attention modules.\n   - A motion adapter handles temporal consistency.\n\n5. **Gray Tile ControlNet**:\n   - Ensures better content preservation and stylization control.\n\nPlease generate:\n 1. A visual diagram that clearly shows the pipeline from video and style image input to stylized video output.\n 2. A separate caption summarizing the figure. Please provide the image and caption separately.",
    "reference_text": "The pipeline of our proposed StyleMaster. We first obtain patch features and image embedding of the style image from CLIP, then we select the patches sharing less similarity with text prompt as texture guidance, and use a global projection module to transform it into global style descriptions. The global projection module is trained with a contrastive dataset constructed by model illusion through contrastive learning. The style information is then injected into the model through the decoupled cross-attention. The motion adapter and gray tile ControlNet are used to enhance dynamic quality and enable content control respectively.",
    "reference_images": [
      "data/diagram/StyleMaster_pipeline.png"
    ],
    "generated_text": "StyleMaster pipeline. A CLIP-based style extractor selects low text-aligned patches from the reference style image, refines them with a Q-Former, and projects them via contrastive learning into global style tokens trained with positives (rearranged anchors) and negatives (model-illusion samples). A spatial–temporal DiT injects text and style through decoupled cross-attention and a motion adapter for temporal consistency. A Gray Tile ControlNet conditions the diffusion to preserve content and control stylization, producing a coherent stylized video.",
    "generated_image": "GPT-thinking/diagram/16.png",
    "gemini_evaluation": {
      "Question": "I am preparing a figure for my paper to illustrate the following framework:\nThis diagram presents the pipeline of the proposed **StyleMaster**, which performs video stylization guided by a style image and a textual description.\n\nThe framework is composed of the following major components:\n\n1. **Input & Output**:\n   - The input is a raw video and a style image (e.g., Van Gogh painting).\n   - The output is a stylized video with artistic consistency.\n\n2. **Style Extraction**:\n   - The style image is encoded using a **CLIP image encoder** to extract patch-level features and image embeddings.\n   - Patches with lower similarity to the accompanying text prompt (e.g., \"A lone penguin walks on a sandy beach at sunset.\") are selected for style representation.\n   - A local selection module (Q-Former) refines these patches.\n   - A **global projection module** transforms them into global style tokens using contrastive learning, enhanced with specially constructed data pairs.\n\n3. **Contrastive Dataset Construction**:\n   - Positive pairs are created by rearranging anchor images.\n   - Negative samples are synthesized via **model illusion** from arbitrary text-to-image models.\n\n4. **Stylization Network (DiT Block)**:\n   - A spatial-temporal diffusion transformer (DiT) applies multiple blocks of self-attention and cross-attention.\n   - Style and text features are injected via decoupled cross-attention modules.\n   - A motion adapter handles temporal consistency.\n\n5. **Gray Tile ControlNet**:\n   - Ensures better content preservation and stylization control.\n\nPlease generate:\n 1. A visual diagram that clearly shows the pipeline from video and style image input to stylized video output.\n 2. A separate caption summarizing the figure. Please provide the image and caption separately.",
      "Evaluation": {
        "Correctness": [
          5,
          "The generated image and text are both factually correct. The diagram accurately represents all the specified components (CLIP encoder, Q-Former, Contrastive Learning, DiT, Motion Adapter, ControlNet) and their relationships as described in the prompt. The text summary is also a precise and accurate description of the pipeline."
        ],
        "Image-Text Coherency": [
          5,
          "The generated text serves as a perfect caption for the generated image. Every element and process mentioned in the text (CLIP-based extractor, Q-Former, contrastive learning, DiT, decoupled cross-attention, etc.) is clearly and correctly depicted in the diagram, and the text follows the logical flow of the visual pipeline."
        ],
        "Content Quality": [
          5,
          "The quality of both the image and text is exceptionally high. The diagram is clear, professionally designed, well-organized, and uses color and icons effectively to enhance understanding. The text is well-written, concise, grammatically correct, and uses appropriate technical terminology."
        ],
        "Human Preference Alignment": [
          5,
          "The response is perfectly aligned with human preference for a technical illustration. The diagram is aesthetically pleasing and easy to interpret, and the text is formal and informative, making it ideal for a scientific paper. There is no undesirable content."
        ],
        "Completeness": [
          5,
          "The response is fully complete. It provides both the requested visual diagram and the separate text caption. All components mentioned in the detailed prompt have been successfully incorporated into both the image and the text, with no omissions."
        ],
        "Content Richness": [
          5,
          "The generated content is very rich. The diagram includes a high level of detail, showing not just the main blocks but also the flow of different types of information (e.g., patch features, style tokens) and the training loop for the projection module. The text, while concise, is packed with all the key technical terms and concepts from the prompt."
        ]
      }
    },
    "gemini_raw_response": "```json\n{\n    \"Question\": \"I am preparing a figure for my paper to illustrate the following framework:\\nThis diagram presents the pipeline of the proposed **StyleMaster**, which performs video stylization guided by a style image and a textual description.\\n\\nThe framework is composed of the following major components:\\n\\n1. **Input & Output**:\\n   - The input is a raw video and a style image (e.g., Van Gogh painting).\\n   - The output is a stylized video with artistic consistency.\\n\\n2. **Style Extraction**:\\n   - The style image is encoded using a **CLIP image encoder** to extract patch-level features and image embeddings.\\n   - Patches with lower similarity to the accompanying text prompt (e.g., \\\"A lone penguin walks on a sandy beach at sunset.\\\") are selected for style representation.\\n   - A local selection module (Q-Former) refines these patches.\\n   - A **global projection module** transforms them into global style tokens using contrastive learning, enhanced with specially constructed data pairs.\\n\\n3. **Contrastive Dataset Construction**:\\n   - Positive pairs are created by rearranging anchor images.\\n   - Negative samples are synthesized via **model illusion** from arbitrary text-to-image models.\\n\\n4. **Stylization Network (DiT Block)**:\\n   - A spatial-temporal diffusion transformer (DiT) applies multiple blocks of self-attention and cross-attention.\\n   - Style and text features are injected via decoupled cross-attention modules.\\n   - A motion adapter handles temporal consistency.\\n\\n5. **Gray Tile ControlNet**:\\n   - Ensures better content preservation and stylization control.\\n\\nPlease generate:\\n\\u20031. A visual diagram that clearly shows the pipeline from video and style image input to stylized video output.\\n\\u20032. A separate caption summarizing the figure. Please provide the image and caption separately.\",\n    \"Evaluation\": {\n        \"Correctness\": [\n            5,\n            \"The generated image and text are both factually correct. The diagram accurately represents all the specified components (CLIP encoder, Q-Former, Contrastive Learning, DiT, Motion Adapter, ControlNet) and their relationships as described in the prompt. The text summary is also a precise and accurate description of the pipeline.\"\n        ],\n        \"Image-Text Coherency\": [\n            5,\n            \"The generated text serves as a perfect caption for the generated image. Every element and process mentioned in the text (CLIP-based extractor, Q-Former, contrastive learning, DiT, decoupled cross-attention, etc.) is clearly and correctly depicted in the diagram, and the text follows the logical flow of the visual pipeline.\"\n        ],\n        \"Content Quality\": [\n            5,\n            \"The quality of both the image and text is exceptionally high. The diagram is clear, professionally designed, well-organized, and uses color and icons effectively to enhance understanding. The text is well-written, concise, grammatically correct, and uses appropriate technical terminology.\"\n        ],\n        \"Human Preference Alignment\": [\n            5,\n            \"The response is perfectly aligned with human preference for a technical illustration. The diagram is aesthetically pleasing and easy to interpret, and the text is formal and informative, making it ideal for a scientific paper. There is no undesirable content.\"\n        ],\n        \"Completeness\": [\n            5,\n            \"The response is fully complete. It provides both the requested visual diagram and the separate text caption. All components mentioned in the detailed prompt have been successfully incorporated into both the image and the text, with no omissions.\"\n        ],\n        \"Content Richness\": [\n            5,\n            \"The generated content is very rich. The diagram includes a high level of detail, showing not just the main blocks but also the flow of different types of information (e.g., patch features, style tokens) and the training loop for the projection module. The text, while concise, is packed with all the key technical terms and concepts from the prompt.\"\n        ]\n    }\n}\n```"
  }
]